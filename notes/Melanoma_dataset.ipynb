{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a592dab344556c81",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "884dfa2c9f2720f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introducción\n",
    "Este cuaderno se centra en el procesamiento y análisis de un conjunto de datos de melanoma. Incluye pasos para la carga de datos, preprocesamiento, entrenamiento del modelo y evaluación.\n",
    "\n",
    "### Indice\n",
    "* [Model](##red-neuronal-simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ee142f79e26fe88f"
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e297458e66d698c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:08.292007400Z",
     "start_time": "2024-01-01T15:18:08.206996500Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "17429aaa5c21af8d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:08.520138700Z",
     "start_time": "2024-01-01T15:18:08.215990600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n",
      "Dispositivo configurado para usar: cuda\n",
      "True\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Configuración del dispositivo CUDA\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo configurado para usar: {device}\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6e9d634414fcb0a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:08.521138800Z",
     "start_time": "2024-01-01T15:18:08.327016100Z"
    }
   },
   "outputs": [],
   "source": [
    "# consts\n",
    "cats = [\"MEL\",\"NV\",\"BCC\",\"AK\",\"BKL\",\"DF\",\"VASC\",\"SCC\",\"UNK\"]\n",
    "\n",
    "base_path_csv = \"./dataset\"\n",
    "base_path_img = \"./dataset\"\n",
    "\n",
    "random_state=1\n",
    "\n",
    "train_percent = .7\n",
    "val_percent = .2\n",
    "test_percent = .1\n",
    "\n",
    "batch_size=32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f446c2ab7e62e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuración\n",
    "Las siguientes celdas importan las librerías necesarias y definen constantes utilizadas a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "51724f93a2becdfb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:08.627162800Z",
     "start_time": "2024-01-01T15:18:08.484705400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de elementos 25331\n",
      "> Head de los datos del csv\n",
      "          image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK\n",
      "0  ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "1  ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "2  ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "3  ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "4  ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(f\"{base_path_csv}/ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "print(\"> Cantidad de elementos\", csv.count(axis=1).size )\n",
    "print(\"> Head de los datos del csv\")\n",
    "print(csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f2c2d80efad22ce3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:08.691177200Z",
     "start_time": "2024-01-01T15:18:08.629163600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Categorias\n",
      "['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "header = list(csv.columns)\n",
    "header.remove(\"image\")\n",
    "\n",
    "print(\"> Categorias\")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "dd487ad7b0d9ea0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.008741700Z",
     "start_time": "2024-01-01T15:18:08.693177700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de filas 25331\n",
      "> Head de los datos en el formato que usa pytorch\n",
      "            img  cat\n",
      "0  ISIC_0000000    1\n",
      "1  ISIC_0000001    1\n",
      "2  ISIC_0000002    0\n",
      "3  ISIC_0000003    1\n",
      "4  ISIC_0000004    0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.DataFrame({\"img\":[], \"cat\": []}, dtype=int)\n",
    "rows_list = []\n",
    "for entry in csv.values:\n",
    "    new_row = {\"img\": entry[0], \"cat\": np.where(entry==1.0)[0][0]-1}\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "data = pd.DataFrame(rows_list)\n",
    "\n",
    "\n",
    "print(\"> Cantidad de filas\", csv.count(axis=1).size)\n",
    "print(\"> Head de los datos en el formato que usa pytorch\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc258a4506fca26f",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f642ef672dfe1d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Carga de Datos y Exploración Inicial\n",
    "El conjunto de datos se carga desde un archivo CSV, y se realiza una exploración inicial para entender su estructura y contenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "6b70f51bf4912f76",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.058023300Z",
     "start_time": "2024-01-01T15:18:09.010741800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de imagenes de cada tipo:\n",
      "[ 4522 12875  3323   867  2624   239   253   628     0]\n"
     ]
    }
   ],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "\n",
    "for elm in data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> Cantidad de imagenes de cada tipo:\")\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "baac2604b9be6feb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.128039100Z",
     "start_time": "2024-01-01T15:18:09.041749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad mínima de imagenes de un tipo:\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "# counter = list(filter(lambda elm: elm > 0, counter))\n",
    "# min_cat_size = min(counter)\n",
    "\n",
    "min_cat_size = data.cat.value_counts().min()\n",
    "\n",
    "print(\"> Cantidad mínima de imagenes de un tipo:\")\n",
    "print(min_cat_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "b75e85454418a3d4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.394549100Z",
     "start_time": "2024-01-01T15:18:09.124037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de elemntos: 1912\n",
      "> Datos equilibrados\n",
      "            img  cat\n",
      "0  ISIC_0032408    0\n",
      "1  ISIC_0056876    0\n",
      "2  ISIC_0033947    0\n",
      "3  ISIC_0072989    0\n",
      "4  ISIC_0054312    0\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(data.groupby(\"cat\").apply(lambda cat: cat.sample(min_cat_size, random_state=random_state)).reset_index(drop=True))\n",
    "\n",
    "print(\"> Cantidad de elemntos:\", data.count(axis=1).size )\n",
    "print(\"> Datos equilibrados\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4a3d5f2b4ec3448b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.485622100Z",
     "start_time": "2024-01-01T15:18:09.390548600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> cantidad de elementos 1912\n",
      "> datos barajados\n",
      "               img  cat\n",
      "301   ISIC_0031379    1\n",
      "137   ISIC_0069254    0\n",
      "1799  ISIC_0058515    7\n",
      "267   ISIC_0029388    1\n",
      "186   ISIC_0070731    0\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1, random_state=random_state)\n",
    "\n",
    "print(\"> cantidad de elementos\", data.count(axis=1).size)\n",
    "print(\"> datos barajados\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f69d52d9c9d54a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocesamiento y Análisis de Datos\n",
    "Estas celdas manejan el preprocesamiento de datos, incluyendo la limpieza, equilibrio y preparación para el aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "52bd2a6b437772a9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.598136600Z",
     "start_time": "2024-01-01T15:18:09.487622600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de imagenes de cada tipo:\n",
      "[239 239 239 239 239 239 239 239   0]\n"
     ]
    }
   ],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "\n",
    "for elm in data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> Cantidad de imagenes de cada tipo:\")\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d41e8af072468566",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.846922100Z",
     "start_time": "2024-01-01T15:18:09.596137600Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.to_csv(f\"{base_path_csv}/balanced_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "675c45665252c0d8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:09.942120400Z",
     "start_time": "2024-01-01T15:18:09.849922400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train 1338\n",
      "               img  cat\n",
      "1073  ISIC_0072864    4\n",
      "104   ISIC_0068581    0\n",
      "450   ISIC_0059722    1\n",
      "505   ISIC_0068758    2\n",
      "1169  ISIC_0070967    4\n",
      "\n",
      "> val 382\n",
      "               img  cat\n",
      "1251  ISIC_0025668    5\n",
      "1498  ISIC_0027563    6\n",
      "1838  ISIC_0025539    7\n",
      "1790  ISIC_0071795    7\n",
      "1551  ISIC_0033254    6\n",
      "\n",
      "> test 192\n",
      "               img  cat\n",
      "1073  ISIC_0072864    4\n",
      "104   ISIC_0068581    0\n",
      "450   ISIC_0059722    1\n",
      "505   ISIC_0068758    2\n",
      "1169  ISIC_0070967    4\n"
     ]
    }
   ],
   "source": [
    "train_data, tmp = train_test_split(data, train_size=train_percent, stratify=data['cat'], shuffle=True, random_state=random_state)\n",
    "val_data, test_data = train_test_split(tmp, test_size=test_percent/(test_percent+val_percent), stratify=tmp['cat'], shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "print(\"> train\", train_data.count(axis=1).size)\n",
    "print(test_data.head())\n",
    "print()\n",
    "\n",
    "print(\"> val\", val_data.count(axis=1).size)\n",
    "print(val_data.head())\n",
    "print()\n",
    "\n",
    "print(\"> test\", test_data.count(axis=1).size)\n",
    "print(test_data.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "65875d972288c8e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:10.059389100Z",
     "start_time": "2024-01-01T15:18:09.946120900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train 1338 0.6997907949790795\n",
      "[167 167 168 167 167 167 167 168   0] (0.12481315396113603)\n",
      "\n",
      "> val 382 0.1997907949790795\n",
      "[48 48 47 48 48 48 48 47  0] (0.1256544502617801)\n",
      "\n",
      "> test 192 0.100418410041841\n",
      "[24 24 24 24 24 24 24 24  0] (0.125)\n"
     ]
    }
   ],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in train_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> train\", train_data.count(axis=1).size, train_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/train_data.count(axis=1).size})\")\n",
    "print()\n",
    "\n",
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in val_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> val\", val_data.count(axis=1).size, val_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/val_data.count(axis=1).size})\")\n",
    "print()\n",
    "\n",
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in test_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> test\", test_data.count(axis=1).size, test_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/test_data.count(axis=1).size})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055c548040e747d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## División de Datos para el Entrenamiento del Modelo\n",
    "El conjunto de datos se divide en conjuntos de entrenamiento, validación y prueba para prepararse para el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "4571fa141d88aa2f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:10.158415Z",
     "start_time": "2024-01-01T15:18:10.053387100Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data.to_csv(f\"{base_path_csv}/train_data.csv\", index=False)\n",
    "# val_data.to_csv(f\"{base_path_csv}/val_data.csv\", index=False)\n",
    "# test_data.to_csv(f\"{base_path_csv}/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed53491ef059f8be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# creacion de un dataset personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "74b025b08473b620",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:10.260899900Z",
     "start_time": "2024-01-01T15:18:10.150412600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import  Dataset\n",
    "image_dir = f\"{base_path_img}/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['img'] + \".jpg\")\n",
    "        image = Image.open(img_name)\n",
    "        label = self.dataframe.iloc[idx]['cat']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c54cfda2a9fbf1f1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.055698300Z",
     "start_time": "2024-01-01T15:18:10.258898700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra 0: Imagen - <class 'torch.Tensor'>, Dimensiones - torch.Size([3, 150, 150]), Etiqueta - 2\n",
      "Muestra 1: Imagen - <class 'torch.Tensor'>, Dimensiones - torch.Size([3, 150, 150]), Etiqueta - 5\n",
      "Muestra 2: Imagen - <class 'torch.Tensor'>, Dimensiones - torch.Size([3, 150, 150]), Etiqueta - 7\n",
      "Muestra 3: Imagen - <class 'torch.Tensor'>, Dimensiones - torch.Size([3, 150, 150]), Etiqueta - 2\n",
      "Muestra 4: Imagen - <class 'torch.Tensor'>, Dimensiones - torch.Size([3, 150, 150]), Etiqueta - 7\n",
      "Media: tensor([0.6479, 0.5226, 0.5249]), Desviación Estándar: tensor([0.2328, 0.2073, 0.2172])\n",
      "Batch de imágenes: torch.Size([32, 3, 150, 150]), Batch de etiquetas: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carga el conjunto de datos de entrenamiento sin ninguna normalización para calcular la media y la desviación estándar\n",
    "unnormalized_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "unnormalized_dataset = CustomDataset(data, image_dir, transform=unnormalized_transform)\n",
    "loader = DataLoader(unnormalized_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "mean_sum = torch.zeros(3)\n",
    "std_sum = torch.zeros(3)\n",
    "n_samples = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    # Asegúrate de que las imágenes están en el rango 0-1\n",
    "    images = images / 255.0 if images.max() > 1 else images\n",
    "\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean = images.mean(dim=[0, 2])\n",
    "    std = images.std(dim=[0, 2])\n",
    "\n",
    "    mean_sum += mean * batch_samples\n",
    "    std_sum += std * batch_samples\n",
    "    n_samples += batch_samples\n",
    "\n",
    "mean = mean_sum / n_samples\n",
    "std = std_sum / n_samples\n",
    "\n",
    "# Transformaciones para el conjunto de entrenamiento\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),             \n",
    "    transforms.RandomRotation(180),            \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Transformaciones para el conjunto de validación y test (sin data augmentation)\n",
    "test_valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_data,image_dir, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_data,image_dir, transform=test_valid_transforms)\n",
    "test_dataset = CustomDataset(test_data, image_dir,transform=test_valid_transforms)\n",
    "\n",
    "# comprobaciones\n",
    "sample_dataset = CustomDataset(train_data, image_dir, transform=train_transform)\n",
    "\n",
    "# Acceder e imprimir las primeras 5 muestras del dataset\n",
    "for i in range(5):\n",
    "    image, label = sample_dataset[i]\n",
    "    print(f\"Muestra {i}: Imagen - {type(image)}, Dimensiones - {image.size()}, Etiqueta - {label}\")\n",
    "\n",
    "# Imprimir los valores de la media y la desviación estándar\n",
    "print(f\"Media: {mean}, Desviación Estándar: {std}\")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True)\n",
    "\n",
    "# Verificar DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch de imágenes: {images.shape}, Batch de etiquetas: {labels.shape}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "86e00ff04666ab8d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.079703600Z",
     "start_time": "2024-01-01T15:18:27.051697500Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fcd7b97d85613",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Red Neuronal simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3195808d0e522f69",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.168724Z",
     "start_time": "2024-01-01T15:18:27.065701100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout25): Dropout(p=0.25, inplace=False)\n",
      "  (dropout50): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=41472, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Capas convolucionales\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Capa Dropout \n",
    "        self.dropout25 = nn.Dropout(0.25)\n",
    "        self.dropout50 = nn.Dropout(0.5)\n",
    "\n",
    "        # Capas Fully connected \n",
    "        # ajustado para 150x150\n",
    "        self.fc1 = nn.Linear(in_features=128 * 18 * 18, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "   \n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "       \n",
    "        x = x.view(-1, 128 *18 * 18)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout50(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "if torch.cuda.device_count() > 1:  model = nn.DataParallel(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9344ab85d911",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparación del Modelo\n",
    "Esta sección prepara el modelo de red neuronal usando PyTorch, incluyendo la definición de la arquitectura del modelo y los cargadores de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d3d3e819c89a605c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.289434300Z",
     "start_time": "2024-01-01T15:18:27.161722400Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e3884d1e16d38",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Funciones de Entrenamiento y Evaluación\n",
    "Aquí se definen las funciones para entrenar el modelo y evaluar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "99dac6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.464538300Z",
     "start_time": "2024-01-01T15:18:27.292435Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, cats in test_loader:\n",
    "            imgs, cats = imgs.to(device), cats.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_loss += criterion(outputs, cats).item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(cats.cpu().numpy())\n",
    "\n",
    "            total += cats.size(0)\n",
    "            correct += (predicted == cats).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f'Accuracy on images: {acc:.2f}%')\n",
    "    print(metrics.classification_report(all_targets, all_preds, zero_division=0))\n",
    "    \n",
    "    return all_preds, all_targets, val_loss / len(test_loader), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=20, min_delta=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.528464Z",
     "start_time": "2024-01-01T15:18:27.465539100Z"
    }
   },
   "id": "c63a25ff37b10f38"
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "8a2f8ca530edeff5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.641589100Z",
     "start_time": "2024-01-01T15:18:27.529464400Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, val_loader, optimizer, criterion, num_epoch, device):\n",
    "    train_accuracy_history = []\n",
    "    val_accuracy_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for imgs, cats in train_loader:\n",
    "            imgs, cats = imgs.to(device), cats.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, cats)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += cats.size(0)\n",
    "            correct += (predicted == cats).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "\n",
    "        # Evaluación en el conjunto de validación\n",
    "        preds, targets, val_loss, val_accuracy = eval(model, val_loader, device, criterion)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    print(\"Finished training\")\n",
    "    return train_accuracy_history, val_accuracy_history, train_loss_history, val_loss_history, preds, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf22adcee1bee40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utilidades Adicionales\n",
    "Funciones de utilidad adicionales, como la detención temprana, se definen en esta sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "97a17b8124f01e9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T15:18:27.657594700Z",
     "start_time": "2024-01-01T15:18:27.641589100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cccb2ef10a97fd",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-01T15:18:27.657594700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on images: 18.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.23      0.25        48\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.23      0.06      0.10        47\n",
      "           3       0.28      0.15      0.19        48\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.00      0.00      0.00        48\n",
      "           6       0.16      0.44      0.23        48\n",
      "           7       0.16      0.57      0.25        47\n",
      "\n",
      "    accuracy                           0.18       382\n",
      "   macro avg       0.14      0.18      0.13       382\n",
      "weighted avg       0.14      0.18      0.13       382\n",
      "\n",
      "Epoch 1: Train Loss: 2.0861, Train Accuracy: 15.99%, Val Loss: 2.0608, Val Accuracy: 18.06%\n",
      "Accuracy on images: 23.30%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.31      0.24        48\n",
      "           1       0.29      0.25      0.27        48\n",
      "           2       0.00      0.00      0.00        47\n",
      "           3       0.22      0.25      0.23        48\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.29      0.17      0.21        48\n",
      "           6       0.25      0.46      0.32        48\n",
      "           7       0.22      0.43      0.29        47\n",
      "\n",
      "    accuracy                           0.23       382\n",
      "   macro avg       0.18      0.23      0.20       382\n",
      "weighted avg       0.18      0.23      0.20       382\n",
      "\n",
      "Epoch 2: Train Loss: 2.0107, Train Accuracy: 20.93%, Val Loss: 1.9728, Val Accuracy: 23.30%\n",
      "Accuracy on images: 21.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.21      0.26        48\n",
      "           1       0.15      0.12      0.13        48\n",
      "           2       0.09      0.02      0.03        47\n",
      "           3       0.23      0.35      0.28        48\n",
      "           4       0.14      0.02      0.04        48\n",
      "           5       0.00      0.00      0.00        48\n",
      "           6       0.23      0.79      0.35        48\n",
      "           7       0.20      0.21      0.21        47\n",
      "\n",
      "    accuracy                           0.22       382\n",
      "   macro avg       0.17      0.22      0.16       382\n",
      "weighted avg       0.17      0.22      0.16       382\n",
      "\n",
      "Epoch 3: Train Loss: 1.9746, Train Accuracy: 20.25%, Val Loss: 1.9230, Val Accuracy: 21.73%\n",
      "Accuracy on images: 25.39%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.10      0.13        48\n",
      "           1       0.26      0.38      0.31        48\n",
      "           2       0.12      0.06      0.08        47\n",
      "           3       0.41      0.19      0.26        48\n",
      "           4       0.12      0.06      0.08        48\n",
      "           5       0.00      0.00      0.00        48\n",
      "           6       0.33      0.88      0.48        48\n",
      "           7       0.21      0.36      0.26        47\n",
      "\n",
      "    accuracy                           0.25       382\n",
      "   macro avg       0.20      0.25      0.20       382\n",
      "weighted avg       0.20      0.25      0.20       382\n",
      "\n",
      "Epoch 4: Train Loss: 1.9086, Train Accuracy: 25.93%, Val Loss: 1.8845, Val Accuracy: 25.39%\n",
      "Accuracy on images: 31.94%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.38      0.27        48\n",
      "           1       0.31      0.54      0.39        48\n",
      "           2       0.17      0.17      0.17        47\n",
      "           3       0.55      0.23      0.32        48\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.24      0.27      0.25        48\n",
      "           6       0.78      0.73      0.75        48\n",
      "           7       0.26      0.23      0.24        47\n",
      "\n",
      "    accuracy                           0.32       382\n",
      "   macro avg       0.31      0.32      0.30       382\n",
      "weighted avg       0.32      0.32      0.30       382\n",
      "\n",
      "Epoch 5: Train Loss: 1.8541, Train Accuracy: 27.88%, Val Loss: 1.8156, Val Accuracy: 31.94%\n",
      "Accuracy on images: 32.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.48      0.29        48\n",
      "           1       0.31      0.48      0.37        48\n",
      "           2       0.15      0.11      0.12        47\n",
      "           3       0.42      0.33      0.37        48\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.27      0.40      0.32        48\n",
      "           6       0.76      0.67      0.71        48\n",
      "           7       0.36      0.11      0.16        47\n",
      "\n",
      "    accuracy                           0.32       382\n",
      "   macro avg       0.31      0.32      0.30       382\n",
      "weighted avg       0.31      0.32      0.30       382\n",
      "\n",
      "Epoch 6: Train Loss: 1.7947, Train Accuracy: 30.64%, Val Loss: 1.7696, Val Accuracy: 32.20%\n",
      "Accuracy on images: 31.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.44      0.27        48\n",
      "           1       0.34      0.56      0.43        48\n",
      "           2       0.24      0.09      0.12        47\n",
      "           3       0.23      0.48      0.31        48\n",
      "           4       0.50      0.02      0.04        48\n",
      "           5       0.32      0.21      0.25        48\n",
      "           6       0.73      0.69      0.71        48\n",
      "           7       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.31       382\n",
      "   macro avg       0.32      0.31      0.27       382\n",
      "weighted avg       0.32      0.31      0.27       382\n",
      "\n",
      "Epoch 7: Train Loss: 1.7644, Train Accuracy: 29.97%, Val Loss: 1.7787, Val Accuracy: 31.15%\n",
      "Accuracy on images: 31.41%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.42      0.31        48\n",
      "           1       0.44      0.50      0.47        48\n",
      "           2       0.17      0.21      0.19        47\n",
      "           3       0.23      0.50      0.32        48\n",
      "           4       0.20      0.02      0.04        48\n",
      "           5       0.31      0.17      0.22        48\n",
      "           6       0.65      0.67      0.66        48\n",
      "           7       0.20      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.31       382\n",
      "   macro avg       0.31      0.31      0.28       382\n",
      "weighted avg       0.31      0.31      0.28       382\n",
      "\n",
      "Epoch 8: Train Loss: 1.7748, Train Accuracy: 31.54%, Val Loss: 1.7652, Val Accuracy: 31.41%\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "num_epochs = 300\n",
    "\n",
    "train_accuracy_history, val_accuracy_history, train_loss_history, val_loss_history,preds,targets= train_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epoch=num_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "try:\n",
    "    torch.save(model, 'modelo_completo.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), 'modelo_estado_88.pth')\n",
    "    print(\"Modelo guardado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(\"Error al guardar el modelo:\", e)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ebca933b926aff8f"
  },
  {
   "cell_type": "markdown",
   "id": "a91045811b60d1d4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Conclusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# generar la matriz de confusión\n",
    "cm = confusion_matrix(targets, preds)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# Gráfico de la precisión de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(val_accuracy_history, label='Validation Accuracy')\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de la pérdida de entrenamiento y validación\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "86e33af808c3791e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c8d41a927e7b397e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
