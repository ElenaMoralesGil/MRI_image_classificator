{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practica 2\n",
    "\n",
    "## Este codigo prepara los datos para usarlos con pytorch (generando un archivo CSV)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9fc39e588b8f97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Este codigo prepara los datos para usarlos con pytorch (generando un archivo CSV)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cff7aa19a3ac2542"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T13:32:45.863673700Z",
     "start_time": "2023-12-29T13:32:45.671424400Z"
    }
   },
   "id": "c1c27ce5733df518"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# consts\n",
    "cats = [\"MEL\",\"NV\",\"BCC\",\"AK\",\"BKL\",\"DF\",\"VASC\",\"SCC\",\"UNK\"]\n",
    "\n",
    "train_percent = .7\n",
    "val_percent = .2\n",
    "test_percent = .1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T13:32:46.152194400Z",
     "start_time": "2023-12-29T13:32:45.688421100Z"
    }
   },
   "id": "f0fee2fdda7496c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sacamos los datos del CSV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51c22ac68fa2b3b9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cantidad de elementos 25331\n",
      "> Head de los datos del csv\n",
      "          image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK\n",
      "0  ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "1  ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "2  ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "3  ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
      "4  ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(\"./dataset/ISIC_2019_Training_GroundTruth.csv\")\n",
    "\n",
    "print(\"> Cantidad de elementos\", csv.count(axis=1).size )\n",
    "print(\"> Head de los datos del csv\")\n",
    "print(csv.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T13:32:46.158195400Z",
     "start_time": "2023-12-29T13:32:45.798445Z"
    }
   },
   "id": "92b3a9a4ed897fc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sacamos las categorias (esto no es necesario)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8c66dbfcb19df1"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Categorias\n",
      "['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "header = list(csv.columns)\n",
    "header.remove(\"image\")\n",
    "\n",
    "print(\"> Categorias\")\n",
    "print(header)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T13:32:46.158195400Z",
     "start_time": "2023-12-29T13:32:45.891680700Z"
    }
   },
   "id": "e9ab4ebbcb46709f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creamos un Dataframe en el formato que requiere pytorch (imagen<string>, categoria<int>)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7946339db283e848"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18192\\83122925.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"img\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"cat\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mentry\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcsv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"img\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mentry\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m,\u001B[0m \u001B[1;34m\"cat\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mentry\u001B[0m\u001B[1;33m==\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"> Cantidad de filas\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcsv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\universidad\\3º año\\FSI\\pythorch\\venv\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6200\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6201\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6202\u001B[0m         ):\n\u001B[0;32m   6203\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6204\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\"img\":[], \"cat\": []}, dtype=int)\n",
    "\n",
    "for entry in csv.values:\n",
    "    data = data.append({\"img\":entry[0] , \"cat\":np.where(entry==1.0)[0][0]-1}, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"> Cantidad de filas\", csv.count(axis=1).size)\n",
    "print(\"> Head de los datos en el formato que usa pytorch\")\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T13:32:46.264219200Z",
     "start_time": "2023-12-29T13:32:46.069036100Z"
    }
   },
   "id": "c481c8f1366be394"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contamos cuantas imagenes hay de cada categoria"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbba44d84a0f5a12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "\n",
    "for elm in data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> Cantidad de imagenes de cada tipo:\")\n",
    "print(counter)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.211206900Z"
    }
   },
   "id": "bc0e11a6b211336"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Averiguamos cual es el tamaño del grupo de imagenes mas pequeño"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61b120a69b300baf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# counter = list(filter(lambda elm: elm > 0, counter))\n",
    "# min_cat_size = min(counter)\n",
    "\n",
    "min_cat_size = data.cat.value_counts().min()\n",
    "\n",
    "print(\"> Cantidad mínima de imagenes de un tipo:\")\n",
    "print(min_cat_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.212207800Z"
    }
   },
   "id": "6950c4077b1fa49c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hacemos que todos los grupos tengan el mismo tamaño, eligiendolos de forma aleatoria"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5a8c447035897d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data.groupby(\"cat\").apply(lambda cat: cat.sample(min_cat_size)).reset_index(drop=True))\n",
    "\n",
    "print(\"> Cantidad de elemntos:\", data.count(axis=1).size )\n",
    "print(\"> Datos equilibrados\")\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.214207400Z"
    }
   },
   "id": "3fe701173aed9a00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Barajamos todas las filas del DataFrame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64cbfd6f6bbb9eed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "\n",
    "print(\"> cantidad de elementos\", data.count(axis=1).size)\n",
    "print(\"> datos barajados\")\n",
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.215209800Z"
    }
   },
   "id": "e68a69ccb1858902"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comprobamos cuantas imagenes hay de cada categoria"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "399dce374c623427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "\n",
    "for elm in data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> Cantidad de imagenes de cada tipo:\")\n",
    "print(counter)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.216208200Z"
    }
   },
   "id": "470557ffc7667896"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Guardamos el resultado en un archivo CSV\n",
    "> Esto para mas eficiencia a la hora de usarlo, mantener siempre el mismo dataset y evitar la sobrerrepresentación de las categorias con mas elementos."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cf1f698e3e19639"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.to_csv(\"./dataset/balanced_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.219210Z"
    }
   },
   "id": "4f03ba0196f184d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ahora vamos a separar los datos en 3 grupos (train, val, test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee388304f3c2ddde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data, tmp = train_test_split(data, train_size=train_percent, stratify=data['cat'], shuffle=True)\n",
    "val_data, test_data = train_test_split(tmp, test_size=test_percent/(test_percent+val_percent), stratify=tmp['cat'], shuffle=True)\n",
    "\n",
    "\n",
    "print(\"> train\", train_data.count(axis=1).size)\n",
    "print(test_data.head())\n",
    "print()\n",
    "\n",
    "print(\"> val\", val_data.count(axis=1).size)\n",
    "print(val_data.head())\n",
    "print()\n",
    "\n",
    "print(\"> test\", test_data.count(axis=1).size)\n",
    "print(test_data.head())\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.220210300Z"
    }
   },
   "id": "b9f77abbcb7df943"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comprobamos cuantas imagenes hay de cada categoria para cada grupo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cbc7321d7550978"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in train_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> train\", train_data.count(axis=1).size, train_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/train_data.count(axis=1).size})\")\n",
    "print()\n",
    "\n",
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in val_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> val\", val_data.count(axis=1).size, val_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/val_data.count(axis=1).size})\")\n",
    "print()\n",
    "\n",
    "counter = np.zeros(len(cats), dtype=int)\n",
    "for elm in test_data.values:\n",
    "    counter[int(elm[1])]+=1\n",
    "\n",
    "print(\"> test\", test_data.count(axis=1).size, test_data.count(axis=1).size/data.count(axis=1).size)\n",
    "print(counter, f\"({counter[0]/test_data.count(axis=1).size})\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.221210Z"
    }
   },
   "id": "1a496339fdf9a0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Barajamos los datos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "984acde72a8618b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1)\n",
    "val_data = val_data.sample(frac=1)\n",
    "test_data = test_data.sample(frac=1)\n",
    "\n",
    "print(\"> train\", train_data.count(axis=1).size)\n",
    "print(train_data.head(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"> val\", val_data.count(axis=1).size)\n",
    "print(val_data.head(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"> test\", test_data.count(axis=1).size)\n",
    "print(test_data.head(), end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.222210200Z"
    }
   },
   "id": "94d10495adeafc71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Guardamos cada uno de los dataframes en un csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726f3f05fdbcf25b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data.to_csv(\"dataset/train_data.csv\", index=False)\n",
    "val_data.to_csv(\"dataset/val_data.csv\", index=False)\n",
    "test_data.to_csv(\"dataset/test_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T13:32:46.224211600Z"
    }
   },
   "id": "2797ef73cf3048b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# consts\n",
    "CRITERION = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d709d2fda39f8c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92805274b9e977ff"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f72604f0b3fb4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, optimizer, criterion, num_epoch, device, history = []):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        running_loss = 0\n",
    "        for i, (imgs, cats) in enumerate(train_loader, 1):\n",
    "            imgs, cats = imgs.to(device), cats.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = criterion(outputs, cats)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if not i % 10:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, Loss: {running_loss/10:.4f}\")\n",
    "                history.append(running_loss/10)\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print(\"Finished training\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce16dbb8f44ec2b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    pred = []\n",
    "    real = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, cats in test_loader:\n",
    "            imgs, cats = imgs.to(device), cats.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            pred.extend(predicted.tolist())\n",
    "            real.extend(cats.tolist())\n",
    "\n",
    "            total += cats.size(0)\n",
    "            correct += (predicted == cats).sum().item()\n",
    "\n",
    "    print(f'Accuracy on test images: {100 * correct / total:.2f}%')\n",
    "    print(metrics.classification_report(pred, real, target_names=dataset.classes))\n",
    "    \n",
    "    #return pred, real"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f699366dbf8a106e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Guia práctica 2 \n",
    "\n",
    "Utilizamos el dataset de kaggle : https://www.kaggle.com/datasets/andrewmvd/isic-2019/data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c037f3acc2c83787"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Definicion de una red convolucional de neuronas, modelo (CNN)\n",
    "\n",
    "Esta red neuronal tiene:\n",
    "\n",
    "3 capas convulutivas cuyo proposito es extraer características de las imagenes:\n",
    "\n",
    "- *conv1*:Toma imágenes de 3 canales (RGB) como entrada y produce 32 mapas de características.\n",
    "- *conv2*: Toma los 32 mapas de características como entrada y produce 64 mapas de características.\n",
    "- *conv3*: Toma los 64 mapas de características como entrada y produce 128 mapas de características.\n",
    "\n",
    "1 capa de Max Pooling cuyo proposito es reducir el ancho y el alto de los mapas de caracteristicas para que disminuya la carga computacional y controlar el sobreajuste.Se aplica despues de cada capa convolucional.\n",
    "\n",
    "2 capas de Dropout que se encarga de prevenir el sobreajuste al establecer aleatoriamente una fracción de las unidades de entrada en 0 durante el entrenamiento.\n",
    "- dropout25: Aplicada después de conv2 y conv3 con una tasa de abandono del 0.25.\n",
    "- dropout50: Aplicada después de la primera capa completamente conectada (fc1) con una tasa de abandono del 0.5.\n",
    "\n",
    "2 Capas Completamente Conectadas (Densas) (Linear):\n",
    "\n",
    "Propósito: Realizar la clasificación en base a las características extraídas por las capas convolucionales.\n",
    "Detalles:\n",
    "fc1: Una capa densa que aplana la salida de la última capa de agrupación en un solo vector y la reduce a 128 características.\n",
    "fc2: La capa densa final que mapea estas 128 características a 8 unidades de salida \n",
    "Método de Avance (forward):\n",
    "\n",
    "Propósito: Define la secuencia en la que los datos de entrada pasan a través de las capas.\n",
    "Detalles: Los datos de entrada pasan por cada capa en el orden definido. Después de la última capa de agrupación, los datos se aplanan y pasan a través de las capas densas.\n",
    "Tamaños de Entrada y Salida:\n",
    "\n",
    "Tamaño de Entrada: El modelo espera imágenes de entrada de tamaño 224x224 después de la transformación RandomResizedCrop.\n",
    "Tamaño de Salida: El modelo produce 8 clases (como se define en fc2).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d7f711e13eeaa0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout25 = nn.Dropout(0.25)\n",
    "        self.dropout50 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # ajustado para 150x150\n",
    "        self.fc1 = nn.Linear(in_features=128 * 18 * 18, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolution, activation and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Apply the second convolution, activation, max pooling and dropout layers\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "        # Apply the third convolution, activation, max pooling and dropout layers\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout25(x)\n",
    "\n",
    "        # Flatten the output for the dense layer\n",
    "        x = x.view(-1, 128 *18 * 18)\n",
    "\n",
    "        # Apply the first dense layer with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout50(x)\n",
    "\n",
    "        # Apply the second dense layer (output layer)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the modified model\n",
    "model = CNN()\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Print the modified model summary\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d82c43bc377723"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "83a9afed9e4336bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Definir Transformaciones de Datos y Data Augmentation\n",
    "\n",
    "Se aplican varias transformaciones a las imagenes de entrenamiento:\n",
    "1. Se escala la imagen a 256x256\n",
    "2. Rotacion aleatoria hasta 180 grados\n",
    "3. Volteo horizontal aleatorio\n",
    "4. Volteo vertical aleatorio\n",
    "5. Recorte Aleatorio: Para simular el enfoque en diferentes partes de la lesión.\n",
    "6. Ajustes de Color: Cambios sutiles en el brillo y el contraste  para simular variaciones de iluminación.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba11cdaeb9f87c9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Rutas a los directorios de datos\n",
    "train_dir = 'path/to/train_data'\n",
    "valid_dir = 'path/to/valid_data'\n",
    "test_dir = 'path/to/test_data'\n",
    "\n",
    "# Carga el conjunto de datos de entrenamiento sin ninguna normalización para calcular la media y la desviación estándar\n",
    "unnormalized_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "unnormalized_dataset = datasets.ImageFolder(root=train_dir, transform=unnormalized_transform)\n",
    "loader = DataLoader(unnormalized_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "mean_sum = torch.zeros(3)\n",
    "std_sum = torch.zeros(3)\n",
    "n_samples = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    images = images.view(images.size(0), 3, -1)\n",
    "    mean, std = torch.std_mean(images, dim=[0, 2])\n",
    "    mean_sum += mean * images.size(0)\n",
    "    std_sum += std * images.size(0)\n",
    "    n_samples += images.size(0)\n",
    "\n",
    "mean = mean_sum / n_samples\n",
    "std = std_sum / n_samples\n",
    "\n",
    "# Transformaciones para el conjunto de entrenamiento\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),             \n",
    "    transforms.RandomRotation(180),            \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Transformaciones para el conjunto de validación y test (sin data augmentation)\n",
    "test_valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Parte de nelson (modificar aqui)\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(root=valid_dir, transform=test_valid_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_valid_transforms)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89ed4be3007db7d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración de DataSetLOaders\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca26b87e45719223"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Crea los DataLoaders para cada conjunto de datos\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b864f772502d8285"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Early Stopping "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c484c3181b8abed9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "# dentro del bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenamiento ...\n",
    "    # Validación ...\n",
    "    \n",
    "    val_loss = # Calcula la pérdida de validación\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3dcf6a2c89b7ef8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cef00f02a11bb53c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
